{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Unet_tfdata.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IozZdZMsBKm0"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s4MK4HWQVzs",
        "outputId": "456ea16c-ff17-4e43-a967-9417d7970ddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQQm7gxCQ54s"
      },
      "source": [
        "!unzip '/content/gdrive/My Drive/new_images.zip' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PktE-n6lQqok"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train2 = pd.read_pickle('/content/gdrive/My Drive/train2_new')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzziUtfrBKnB"
      },
      "source": [
        "Successfully created augmented images and their encoded pixels. train2 contains the names and the encoded pixels of all images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P_KV2ssWjU8",
        "outputId": "239c0b21-50af-4ea3-da9c-a4e2727dff02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tqdm import tqdm \n",
        "\n",
        "for i in tqdm(range(len(train2))):\n",
        "  for j in range(1,5):\n",
        "    if type(train2['e'+str(j)].iloc[i]) is not str:\n",
        "      train2['e'+str(j)][i] = train2['e'+str(j)].iloc[i][0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13377/13377 [00:03<00:00, 3408.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ7Mlp4rMFtW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test = train_test_split(train2, test_size=0.2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6z3ORmPBKnK"
      },
      "source": [
        "# Tensorflow tf.data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqW5WlqsZet7"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def masks(encoded_pixels):\n",
        "    counts=[]\n",
        "    mask=np.zeros((256*1600), dtype=np.int8) \n",
        "    pre_mask=np.asarray([int(point) for point in encoded_pixels.split()])\n",
        "    for index,count in enumerate(pre_mask):\n",
        "        if(index%2!=0):\n",
        "            counts.append(count)\n",
        "    i=0\n",
        "    for index,pixel in enumerate(pre_mask):\n",
        "        if(index%2==0):\n",
        "            if(i==len(counts)):\n",
        "                break\n",
        "            mask[pixel:pixel+counts[i]]=1\n",
        "            i+=1\n",
        "    mask=np.reshape(mask,(1600,256))\n",
        "    mask=cv2.resize(mask,(256,1600)).T\n",
        "    return mask\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def preprocess(file_path):\n",
        "    # you should decode bytes type to string type\n",
        "    img_name = (file_path.numpy()[0].decode('UTF-8'))\n",
        "    img_name = 'images/'+img_name\n",
        "    img = cv2.imread(img_name)/255\n",
        "    img = cv2.resize(img,(1600,256))\n",
        "    mask = np.empty((256,1600,4),dtype=np.int8)\n",
        "    for i in range(4):\n",
        "      mask[:,:,i] = masks(file_path.numpy()[i+1].decode('UTF-8'))\n",
        "    mask = tf.convert_to_tensor(mask)\n",
        "    return img,mask"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cnnRdogGlto"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X_train.values)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(X_test.values)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiFqPlqOAqei"
      },
      "source": [
        "train_ds = train_dataset.map(lambda x: tf.py_function(preprocess, [x], [tf.float64,tf.int8]),num_parallel_calls=5)\n",
        "test_ds = test_dataset.map(lambda x: tf.py_function(preprocess, [x], [tf.float64,tf.int8]),num_parallel_calls=5)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUp9Qr2OVz5Q",
        "outputId": "107a87d9-7091-4f0f-feae-dc204e381cf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "\n",
        "# for reference about the BUFFER_SIZE in shuffle:\n",
        "# https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = {\"train\": train_ds, \"val\": test_ds}\n",
        "\n",
        "# -- Train Dataset --#\n",
        "dataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=42)\n",
        "dataset['train'] = dataset['train'].repeat()\n",
        "dataset['train'] = dataset['train'].batch(BATCH_SIZE)\n",
        "dataset['train'] = dataset['train'].prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#-- Validation Dataset --#\n",
        "dataset['val'] = dataset['val'].repeat()\n",
        "dataset['val'] = dataset['val'].batch(BATCH_SIZE)\n",
        "dataset['val'] = dataset['val'].prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(dataset['train'])\n",
        "print(dataset['val'])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (<unknown>, <unknown>), types: (tf.float64, tf.int8)>\n",
            "<PrefetchDataset shapes: (<unknown>, <unknown>), types: (tf.float64, tf.int8)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwfeuJ_sMPOT",
        "outputId": "fbe95d36-607f-487c-d910-57a2b25dc26b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip uninstall keras -y\n",
        "! pip install segmentation-models"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping keras as it is not installed.\u001b[0m\n",
            "Requirement already satisfied: segmentation-models in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.8)\n",
            "Requirement already satisfied: efficientnet==1.0.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: image-classifiers==1.0.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.18.5)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.5)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0JahP2hBKne"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVrWDI_PMLU3"
      },
      "source": [
        "# Dice similarity coefficient loss, brought to you by: https://github.com/nabsabraham/focal-tversky-unet\n",
        "from tensorflow.keras import backend as K\n",
        "# https://www.kaggle.com/xhlulu/severstal-simple-keras-u-net-boilerplate\n",
        "\n",
        "# COMPETITION METRIC\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI37bWpRK5cs",
        "outputId": "e8ca53cd-135e-4d42-e0b2-1804f00f5511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "SM_FRAMEWORK=tf.keras\n",
        "import segmentation_models as sm\n",
        "from segmentation_models import Unet\n",
        "from segmentation_models import get_preprocessing\n",
        "from segmentation_models.losses import DiceLoss\n",
        "from segmentation_models.metrics import iou_score,f1_score,Recall\n",
        "import tensorflow\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add, Flatten\n",
        "from tensorflow import reduce_sum\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "WIDTH=1600\n",
        "HEIGHT=256 \n",
        "\n",
        "import segmentation_models as sm\n",
        "network = 'resnet34'\n",
        "process_input = get_preprocessing(network)\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(lr=0.001)\n",
        "\n",
        "model = Unet(network,input_shape = (256, 1600, 3),classes=4,activation='sigmoid',encoder_freeze=True)\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=[dice_coef])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Segmentation Models: using `tf.keras` framework.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nic31Ue6MEop",
        "outputId": "7ca3f739-5697-43d0-d598-a3fe370b3d10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback,ModelCheckpoint\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "callback_list=[]\n",
        "%load_ext tensorboard\n",
        "log_dir=\"Model-1-logs\"\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph\n",
        "=True,write_grads=True)\n",
        "callback_list.append(tensorboard_callback)\n",
        "\n",
        "filepath=\"Model-Unet-weights-v2.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=0, save_best_only=True,\n",
        "mode='auto')\n",
        "callback_list.append(checkpoint)\n",
        "\n",
        "callback_list.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2))\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8,patience=1, min_lr=0.001)\n",
        "callback_list.append(reduce_lr)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REeYDpdW5N5i"
      },
      "source": [
        "model.load_weights('/content/gdrive/My Drive/Model-Unet-weights.hdf5')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCs5iRbH5rSC"
      },
      "source": [
        "Increased size of train data and randomized train test split to increase randomness in data and avoid overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5z_apTSMwhu",
        "outputId": "1731d5c5-45e4-4aa0-8006-79f7ea7f9b91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(dataset['train'],validation_data = dataset['val'],epochs = 5, callbacks=callback_list,steps_per_epoch=len(X_train)//10,validation_steps=len(X_test)//10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "   1/1070 [..............................] - ETA: 0s - loss: 0.0100 - dice_coef: 0.7047WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "   2/1070 [..............................] - ETA: 5:54 - loss: 0.0109 - dice_coef: 0.7227WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2455s vs `on_train_batch_end` time: 0.4175s). Check your callbacks.\n",
            "1070/1070 [==============================] - 607s 567ms/step - loss: 0.0133 - dice_coef: 0.6651 - val_loss: 0.0123 - val_dice_coef: 0.6639\n",
            "Epoch 2/5\n",
            "1070/1070 [==============================] - 606s 566ms/step - loss: 0.0131 - dice_coef: 0.6675 - val_loss: 0.0124 - val_dice_coef: 0.6628\n",
            "Epoch 3/5\n",
            "1070/1070 [==============================] - 607s 567ms/step - loss: 0.0130 - dice_coef: 0.6715 - val_loss: 0.0125 - val_dice_coef: 0.6682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7044da67b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u-jPdNKBKnx"
      },
      "source": [
        "No overfit is seen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoleU4WzubF0"
      },
      "source": [
        "!cp '/content/Model-Unet-weights-v2.hdf5' '/content/gdrive/My Drive'"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}